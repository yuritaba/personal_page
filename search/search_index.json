{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Yuri Tabacof","text":""},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 05/09/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"projeto/main/","title":"Main","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"projeto%201/main/","title":"Projeto 1 - Classification","text":""},{"location":"projeto%201/main/#1-dataset-selection","title":"1. Dataset Selection","text":"<p>Nome do dataset: Default of Credit Card Clients (Taiwan) Fonte: UCI Machine Learning Repository URL: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients Tamanho: 30.000 registros, 23 vari\u00e1veis preditoras + 1 vari\u00e1vel-alvo  </p> <p>Tarefa: Classifica\u00e7\u00e3o bin\u00e1ria \u2014 prever se um cliente entrar\u00e1 em default (inadimpl\u00eancia) no m\u00eas seguinte.</p> <p>Justificativa da escolha: - O problema \u00e9 realista e relevante no contexto financeiro (risco de cr\u00e9dito). - Cont\u00e9m dados mistos (num\u00e9ricos e categ\u00f3ricos), o que torna o pr\u00e9-processamento e o aprendizado mais desafiadores e instrutivos. - Possui volume adequado (&gt;1.000 amostras e &gt;5 atributos), atendendo aos requisitos do projeto. - Apresenta classes desbalanceadas, o que permite discutir m\u00e9tricas alternativas \u00e0 acur\u00e1cia e estrat\u00e9gias de balanceamento. - \u00c9 uma base p\u00fablica e amplamente utilizada em pesquisa aplicada, sem ser uma das cl\u00e1ssicas proibidas (Titanic, Iris, Wine etc.).</p>"},{"location":"projeto%201/main/#2-dataset-explanation","title":"2. Dataset Explanation","text":""},{"location":"projeto%201/main/#contexto-e-descricao","title":"Contexto e Descri\u00e7\u00e3o","text":"<p>O dataset cont\u00e9m informa\u00e7\u00f5es de 30.000 clientes de cart\u00e3o de cr\u00e9dito em Taiwan. Cada registro representa um cliente, e a vari\u00e1vel-alvo indica se ele deu default no m\u00eas seguinte (<code>default.payment.next.month</code> = 1) ou n\u00e3o (= 0). Os atributos incluem dados demogr\u00e1ficos, financeiros e hist\u00f3ricos de pagamento.</p>"},{"location":"projeto%201/main/#variaveis","title":"Vari\u00e1veis","text":"<p>Demogr\u00e1ficas - <code>SEX</code>: G\u00eanero (1 = masculino, 2 = feminino) - <code>EDUCATION</code>: Grau de instru\u00e7\u00e3o (1 = p\u00f3s-gradua\u00e7\u00e3o, 2 = gradua\u00e7\u00e3o, 3 = ensino m\u00e9dio, 4/0/5/6 = outros) - <code>MARRIAGE</code>: Estado civil (1 = casado, 2 = solteiro, 3/0 = outros) - <code>AGE</code>: Idade (anos)</p> <p>Financeiras - <code>LIMIT_BAL</code>: Limite total de cr\u00e9dito (em NT$)</p> <p>Hist\u00f3rico de pagamento (\u00faltimos 6 meses) - <code>PAY_0</code>, <code>PAY_2</code>, <code>PAY_3</code>, <code>PAY_4</code>, <code>PAY_5</code>, <code>PAY_6</code> \u2014 Status de pagamento (valores inteiros, onde \u22122/\u22121/0 indicam pagos em dia, e 1, 2, ... indicam atraso em meses)</p> <p>Faturas mensais - <code>BILL_AMT1</code> a <code>BILL_AMT6</code> \u2014 Valores das faturas nos seis meses anteriores</p> <p>Pagamentos mensais - <code>PAY_AMT1</code> a <code>PAY_AMT6</code> \u2014 Valores pagos nos seis meses anteriores</p> <p>Alvo - <code>default.payment.next.month</code>: 0 = n\u00e3o entrou em default; 1 = entrou em default</p>"},{"location":"projeto%201/main/#tipos-de-dados","title":"Tipos de dados","text":"<ul> <li>Num\u00e9ricos cont\u00ednuos: <code>LIMIT_BAL</code>, <code>AGE</code>, <code>BILL_AMT*</code>, <code>PAY_AMT*</code> </li> <li>Num\u00e9ricos discretos: <code>PAY_*</code> </li> <li>Categ\u00f3ricos: <code>SEX</code>, <code>EDUCATION</code>, <code>MARRIAGE</code> </li> <li>Bin\u00e1rio (target): <code>default.payment.next.month</code></li> </ul>"},{"location":"projeto%201/main/#principais-desafios","title":"Principais Desafios","text":"<ul> <li>Desbalanceamento: apenas ~22% dos clientes est\u00e3o em default.  </li> <li>Categorias inv\u00e1lidas: <code>EDUCATION</code> e <code>MARRIAGE</code> cont\u00eam c\u00f3digos inconsistentes.  </li> <li>Outliers: valores muito altos em <code>BILL_AMT*</code> e <code>PAY_AMT*</code>.  </li> <li>Multicolinearidade: alta correla\u00e7\u00e3o entre s\u00e9ries temporais (meses consecutivos).  </li> <li>Escalas muito diferentes: necessidade de normaliza\u00e7\u00e3o antes do treino do MLP.</li> </ul>"},{"location":"projeto%201/main/#estatisticas-e-visualizacoes-planejadas","title":"Estat\u00edsticas e Visualiza\u00e7\u00f5es (planejadas)","text":"<ul> <li>Distribui\u00e7\u00e3o do alvo (<code>default.payment.next.month</code>)  </li> <li>Histogramas de <code>LIMIT_BAL</code> e <code>AGE</code> </li> <li>Heatmap de correla\u00e7\u00e3o entre vari\u00e1veis num\u00e9ricas  </li> <li>Tabela resumo de m\u00e9dias, desvios e amplitudes  </li> </ul>"},{"location":"projeto%201/main/#consideracoes-eticas","title":"Considera\u00e7\u00f5es \u00c9ticas","text":"<p>Alguns atributos (g\u00eanero, estado civil, escolaridade) podem introduzir vi\u00e9s algor\u00edtmico. Discuss\u00f5es sobre fairness e mitiga\u00e7\u00e3o de vi\u00e9s s\u00e3o pertinentes ao interpretar os resultados.</p>"},{"location":"projeto%201/main/#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization","text":""},{"location":"projeto%201/main/#estrutura-e-natureza-dos-dados","title":"Estrutura e Natureza dos Dados","text":"<p>Os dados utilizados neste projeto representam informa\u00e7\u00f5es de clientes de cart\u00e3o de cr\u00e9dito, com atributos que descrevem aspectos demogr\u00e1ficos, financeiros e comportamentais. Cada linha do dataset corresponde a um cliente, e cada coluna representa uma feature (atributo), como limite de cr\u00e9dito, idade, estado civil ou hist\u00f3rico de pagamento. Essa estrutura, em forma de matriz de atributos, \u00e9 a base para a aplica\u00e7\u00e3o de t\u00e9cnicas de aprendizado supervisionado, em que cada exemplo possui um conjunto de entradas (features) e uma sa\u00edda (r\u00f3tulo).</p> <p>As vari\u00e1veis do conjunto de dados podem ser classificadas em dois tipos principais:</p> <ul> <li>Num\u00e9ricas: representam valores cont\u00ednuos ou discretos, como <code>LIMIT_BAL</code> (limite de cr\u00e9dito) e <code>AGE</code> (idade);  </li> <li>Categ\u00f3ricas: representam valores qualitativos, como <code>SEX</code>, <code>EDUCATION</code> e <code>MARRIAGE</code>.</li> </ul> <p>Essa distin\u00e7\u00e3o \u00e9 fundamental, pois cada tipo de dado requer um tratamento espec\u00edfico para que o modelo de aprendizado consiga interpretar corretamente as informa\u00e7\u00f5es.</p>"},{"location":"projeto%201/main/#limpeza-e-qualidade-dos-dados","title":"Limpeza e Qualidade dos Dados","text":"<p>A qualidade dos dados \u00e9 essencial para o desempenho de qualquer modelo de aprendizado de m\u00e1quina. Durante a etapa de limpeza, foram verificados problemas comuns como valores ausentes, duplicatas e inconsist\u00eancias.</p> <ul> <li>Valores ausentes: n\u00e3o foram encontrados no conjunto de dados.  </li> <li>Duplicatas: nenhuma linha duplicada foi identificada.  </li> <li>Inconsist\u00eancias: categorias incorretas, como <code>EDUCATION = 0, 5, 6</code> e <code>MARRIAGE = 0</code>, foram recategorizadas como \u201cOutros\u201d, garantindo consist\u00eancia nos dados.  </li> <li>Valores inv\u00e1lidos: apenas uma amostra foi removida por conter um valor incorreto na vari\u00e1vel alvo (<code>default_payment_next_month</code>).</li> </ul> <p>Ap\u00f3s a limpeza, o dataset permaneceu com 30.000 amostras v\u00e1lidas, todas completas e sem inconsist\u00eancias estruturais.</p>"},{"location":"projeto%201/main/#pre-processamento-e-transformacao-dos-dados","title":"Pr\u00e9-processamento e Transforma\u00e7\u00e3o dos Dados","text":"<p>Como o modelo de aprendizado requer entradas num\u00e9ricas, as vari\u00e1veis categ\u00f3ricas foram convertidas em formato num\u00e9rico por meio da t\u00e9cnica de One-Hot Encoding, criando uma coluna para cada categoria poss\u00edvel de <code>SEX</code>, <code>EDUCATION</code> e <code>MARRIAGE</code>. Esse processo garante que o modelo interprete corretamente diferen\u00e7as qualitativas entre categorias, sem atribuir ordens artificiais a elas.</p> <p>Em seguida, os dados num\u00e9ricos foram normalizados para uma escala comum, de forma que todas as vari\u00e1veis contribuam igualmente durante o treinamento da rede neural. Esse procedimento \u00e9 essencial para evitar que atributos com valores mais altos dominem a fun\u00e7\u00e3o de custo do modelo.</p> <p>Por fim, o conjunto de dados foi dividido em tr\u00eas subconjuntos: - Treino (60%) \u2013 usado para o aprendizado do modelo; - Valida\u00e7\u00e3o (20%) \u2013 usado para ajuste de par\u00e2metros; - Teste (20%) \u2013 usado para avaliar a capacidade de generaliza\u00e7\u00e3o.</p> <p>A divis\u00e3o foi feita de forma estratificada, mantendo a propor\u00e7\u00e3o original das classes (<code>default = 1</code> e <code>non-default = 0</code>) em todos os conjuntos.</p>"},{"location":"projeto%201/main/#resumo-do-processo-de-preparacao","title":"Resumo do Processo de Prepara\u00e7\u00e3o","text":"Etapa A\u00e7\u00e3o Realizada Verifica\u00e7\u00e3o de valores ausentes Nenhum valor ausente encontrado Remo\u00e7\u00e3o de duplicatas Nenhuma duplicata detectada Corre\u00e7\u00e3o de categorias inv\u00e1lidas Reclassifica\u00e7\u00e3o de valores fora do intervalo v\u00e1lido Exclus\u00e3o de valores incorretos 1 registro removido Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas One-Hot Encoding aplicado Normaliza\u00e7\u00e3o Escalonamento dos atributos num\u00e9ricos Divis\u00e3o do dataset 60% treino, 20% valida\u00e7\u00e3o, 20% teste (estratificado) <p>Esses procedimentos asseguraram que o dataset estivesse limpo, consistente e devidamente estruturado, seguindo as boas pr\u00e1ticas de qualidade, balanceamento e padroniza\u00e7\u00e3o de dados recomendadas em Machine Learning.</p>"},{"location":"projeto%201/main/#4-implementacao-do-mlp-numpy","title":"4. Implementa\u00e7\u00e3o do MLP (NumPy)","text":""},{"location":"projeto%201/main/#implementacao","title":"Implementa\u00e7\u00e3o","text":"<p>Implementamos um MLP do zero, usando apenas NumPy (produto matricial, ativa\u00e7\u00f5es, softmax, cross\u2011entropy, backprop e atualiza\u00e7\u00e3o dos pesos). O objetivo \u00e9 classificar inadimpl\u00eancia (<code>default_payment_next_month</code>) a partir dos dados j\u00e1 limpos/normalizados.</p>"},{"location":"projeto%201/main/#arquitetura-e-treino","title":"Arquitetura e treino","text":"<p><pre><code>Entrada (d_in) \u2192 ReLU(64) \u2192 ReLU(32) \u2192 Softmax(2)\n</code></pre> - Ativa\u00e7\u00f5es: ReLU nas camadas escondidas; Softmax na sa\u00edda. - Loss: Cross-Entropy (com pesos de classe para desbalanceamento) + L2. - Otimiza\u00e7\u00e3o: SGD mini-batch com momentum, learning rate decay e early stopping. - Limiar de decis\u00e3o: escolhido na valida\u00e7\u00e3o para m\u00e1ximo F1.</p>"},{"location":"projeto%201/main/#hiperparametros","title":"Hiperpar\u00e2metros","text":"<ul> <li>Camadas escondidas: (64, 32) </li> <li>Batch size: 256 </li> <li>\u00c9pocas m\u00e1x.: 60 (com early stopping, paci\u00eancia=8)  </li> <li>Learning rate inicial: 1e\u20112 (decai 0.9 a cada 5 \u00e9pocas)  </li> <li>L2 (weight decay): 1e\u20114 </li> <li>Seed: 42 </li> <li>Threshold (val, melhor F1): 0.47</li> </ul>"},{"location":"projeto%201/main/#resultados","title":"Resultados","text":"Conjunto Acc Precision Recall F1 ROC\u2011AUC Treino 0.7533 0.4603 0.6660 0.5443 0.7979 Valida\u00e7\u00e3o 0.7367 0.4325 0.6104 0.5062 0.7567 Teste 0.7418 0.4425 0.6443 0.5247 0.7750 <ul> <li>Acc (Accuracy): Propor\u00e7\u00e3o total de acertos \u2014 ou seja, quantos exemplos o modelo classificou corretamente (positivos e negativos) entre todos os exemplos. F\u00f3rmula: (VP + VN) / Total</li> <li>Precision: Propor\u00e7\u00e3o de exemplos classificados como positivos que realmente s\u00e3o positivos. Mede a confiabilidade das previs\u00f5es positivas. F\u00f3rmula: VP / (VP + FP)</li> <li>Recall (Sensibilidade): Propor\u00e7\u00e3o de exemplos positivos reais que o modelo conseguiu capturar. Mede a capacidade de detectar inadimplentes. F\u00f3rmula: VP / (VP + FN)</li> <li>F1-score: M\u00e9dia harm\u00f4nica entre Precision e Recall. Equilibra os dois em uma \u00fanica m\u00e9trica, especialmente \u00fatil com classes desbalanceadas. F\u00f3rmula: 2 \u00b7 (Prec \u00b7 Rec) / (Prec + Rec)</li> <li>ROC\u2011AUC: \u00c1rea sob a curva ROC (Receiver Operating Characteristic), que mede a capacidade do modelo de separar classes. Quanto mais pr\u00f3ximo de 1, melhor a separa\u00e7\u00e3o entre inadimplentes e n\u00e3o inadimplentes.</li> </ul> <p>Observa\u00e7\u00e3o. Em dados desbalanceados, otimizar F1/Recall (via threshold) pode reduzir a accuracy em rela\u00e7\u00e3o ao baseline que sempre prev\u00ea a classe majorit\u00e1ria. Aqui priorizamos recuperar mais inadimplentes mantendo AUC e F1 s\u00f3lidos.</p>"},{"location":"projeto%201/main/#codigo-completo","title":"C\u00f3digo Completo","text":"<p> Editor (session: default) Run <pre>\n# projeto1.py\n# Data Cleaning &amp; Normalization + MLP from scratch (NumPy)\n# \u2714\ufe0f Rode com \"Run Python File\" (\u25b6) no VS Code, sem passar argumentos.\n# \u2714\ufe0f Passos:\n#     1) Auto-descobre o Excel (.xls/.xlsx), limpa e normaliza (One-Hot, split, z-score)\n#     2) Treina um MLP (NumPy) com ReLU + Softmax, CE ponderada, mini-batch SGD + Momentum, L2, early stopping\n#     3) Ajusta threshold pelo melhor F1 na valida\u00e7\u00e3o e imprime m\u00e9tricas\n#\n# Requisitos:\n#   pip install numpy pandas scikit-learn xlrd\n#\n# Sa\u00eddas (padr\u00e3o: ./processed):\n#   - X_train.npy, y_train.npy, X_val.npy, y_val.npy, X_test.npy, y_test.npy\n#   - scaler_mu.npy, scaler_sd.npy, columns.json\n#   - class_dist_before.json, class_dist_splits.json\n#   - cleaned_full_sample.csv (amostra 5%)\n#   - training_curves.csv (perdas por \u00e9poca)\n#   - final_metrics.json (m\u00e9tricas + threshold escolhido)\n#\n# ------------------------------------------------------------------------------\n#                                IMPORTS\n# ------------------------------------------------------------------------------\n\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\n# ------------------------------------------------------------------------------\n#                                CONFIG\n# ------------------------------------------------------------------------------\n\n# (A) Cleaning\nSEED = 42\nWINSOR_LO = 1.0\nWINSOR_HI = 99.0\nSAMPLE_CSV_FRAC = 0.05\nOUTDIR_NAME = \"processed\"\n\nPOSSIBLE_DATAFILES = [\n    \"default of credit card clients.xls\",\n    \"default_of_credit_card_clients.xls\",\n    \"default of credit card clients.xlsx\",\n    \"default_of_credit_card_clients.xlsx\",\n    \"UCI_Credit_Card.xls\",\n    \"UCI_Credit_Card.xlsx\",\n]\n\nTARGET_CANDIDATES = [\n    \"default_payment_next_month\",\n    \"default.payment.next.month\",\n    \"default payment next month\",\n    \"y\",\n]\nID_CANDIDATES = [\"id\", \"unnamed:_0\"]\nCAT_COLS_RAW = [\"sex\", \"education\", \"marriage\"]\n\n# (B) MLP Hyperparameters\nMLP_LAYERS = (64, 32)     # camadas escondidas\nMLP_L2 = 1e-4             # weight decay\nLR_INIT = 1e-2            # learning rate inicial\nBATCH_SIZE = 256\nEPOCHS = 60\nPATIENCE = 8              # early stopping (\u00e9pocas sem melhora em val)\nLR_DECAY = 0.9            # multiplicador a cada LR_DECAY_EVERY \u00e9pocas\nLR_DECAY_EVERY = 5\nMOMENTUM = 0.9            # SGD momentum\n\n# ------------------------------------------------------------------------------\n#                             HELPER FUNCTIONS\n# ------------------------------------------------------------------------------\n\ndef print_header(title):\n    print(\"\\n\" + \"=\" * 80)\n    print(title)\n    print(\"=\" * 80)\n\n\ndef normalize_columns(cols):\n    out = []\n    for c in cols:\n        c2 = str(c).strip()\n        c2 = c2.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"-\", \" \").replace(\"/\", \" \")\n        c2 = \" \".join(c2.split())\n        c2 = c2.lower().replace(\" \", \"_\")\n        out.append(c2)\n    return out\n\n\ndef try_read_excel(path: Path) -&gt; pd.DataFrame:\n    for kw in [\n        dict(header=0, engine=\"xlrd\"),\n        dict(header=1, engine=\"xlrd\"),\n        dict(header=0),\n        dict(header=1),\n    ]:\n        try:\n            return pd.read_excel(path, sheet_name=0, **kw)\n        except Exception:\n            continue\n    return pd.read_excel(path)\n\n\ndef find_first_present(candidates, cols):\n    for c in candidates:\n        if c in cols:\n            return c\n    return None\n\n\ndef auto_find_datafile(script_dir: Path) -&gt; Path:\n    for name in POSSIBLE_DATAFILES:\n        p = script_dir / name\n        if p.exists():\n            return p\n    for p in sorted(script_dir.glob(\"*.xls\")) + sorted(script_dir.glob(\"*.xlsx\")):\n        return p\n    raise FileNotFoundError(\n        \"\u274c Nenhum arquivo .xls/.xlsx encontrado. Coloque o Excel na mesma pasta do .py.\"\n    )\n\n\n# ------------------------------------------------------------------------------\n#                       SECTION 3 \u2014 CLEANING &amp; NORMALIZATION\n# ------------------------------------------------------------------------------\n\ndef run_cleaning_and_save(script_dir: Path):\n    outdir = script_dir / OUTDIR_NAME\n    outdir.mkdir(parents=True, exist_ok=True)\n\n    # 1) Carregar .xls/.xlsx automaticamente\n    print_header(\"[1] Carregando planilha de dados (auto-descoberta)\")\n    data_path = auto_find_datafile(script_dir)\n    print(f\"Arquivo detectado: {data_path.name}\")\n    df = try_read_excel(data_path)\n    print(f\"Shape bruto lido: {df.shape}\")\n\n    df.columns = normalize_columns(df.columns)\n    print(\"Colunas normalizadas (primeiras 15):\", list(df.columns)[:15])\n\n    df = df.dropna(how=\"all\")\n    print(f\"Shape ap\u00f3s remover linhas 100% vazias: {df.shape}\")\n\n    # Detectar schema X1..X23 + Y e renomear\n    cols = set(df.columns)\n    x_schema_ok = {\n        \"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x11\",\n        \"x12\",\"x13\",\"x14\",\"x15\",\"x16\",\"x17\",\"x18\",\"x19\",\"x20\",\"x21\",\"x22\",\"x23\",\"y\"\n    }.issubset(cols)\n\n    if x_schema_ok:\n        mapping = {\n            \"unnamed:_0\": \"id\",\n            \"x1\":  \"limit_bal\",\n            \"x2\":  \"sex\",\n            \"x3\":  \"education\",\n            \"x4\":  \"marriage\",\n            \"x5\":  \"age\",\n            \"x6\":  \"pay_0\",\n            \"x7\":  \"pay_2\",\n            \"x8\":  \"pay_3\",\n            \"x9\":  \"pay_4\",\n            \"x10\": \"pay_5\",\n            \"x11\": \"pay_6\",\n            \"x12\": \"bill_amt1\",\n            \"x13\": \"bill_amt2\",\n            \"x14\": \"bill_amt3\",\n            \"x15\": \"bill_amt4\",\n            \"x16\": \"bill_amt5\",\n            \"x17\": \"bill_amt6\",\n            \"x18\": \"pay_amt1\",\n            \"x19\": \"pay_amt2\",\n            \"x20\": \"pay_amt3\",\n            \"x21\": \"pay_amt4\",\n            \"x22\": \"pay_amt5\",\n            \"x23\": \"pay_amt6\",\n            \"y\":   \"default_payment_next_month\",\n        }\n        df = df.rename(columns={k: v for k, v in mapping.items() if k in df.columns})\n        if \"id\" not in df.columns:\n            if \"unnamed:_0\" in df.columns:\n                df = df.rename(columns={\"unnamed:_0\": \"id\"})\n            else:\n                df.insert(0, \"id\", np.arange(len(df)))\n        print(\"[PATCH] Detectado schema X1..X23 + Y. Colunas renomeadas para nomes oficiais.\")\n        print(\"Colunas (amostra):\", list(df.columns)[:15])\n\n    # detectar alvo e id\n    target_col = find_first_present(TARGET_CANDIDATES, df.columns)\n    if target_col is None:\n        raise KeyError(f\"Coluna alvo n\u00e3o encontrada. Esperado uma entre: {TARGET_CANDIDATES}.\")\n    id_col = find_first_present(ID_CANDIDATES, df.columns)\n    print(f\"Alvo: {target_col} | ID: {id_col if id_col else '(n\u00e3o h\u00e1 \u2014 usarei \u00edndice)'}\")\n\n    # coer\u00e7\u00e3o de tipos (alvo e categ\u00f3ricas)\n    df[target_col] = pd.to_numeric(df[target_col], errors=\"coerce\")\n    n_nan_target = int(df[target_col].isna().sum())\n    if n_nan_target &gt; 0:\n        print(f\"[PATCH] Removendo {n_nan_target} linhas com alvo inv\u00e1lido.\")\n        df = df.dropna(subset=[target_col])\n    df[target_col] = df[target_col].astype(int)\n    for c in [\"sex\", \"education\", \"marriage\"]:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n\n    # 2) Duplicados\n    print_header(\"[2] Remo\u00e7\u00e3o de duplicados\")\n    n_before = len(df)\n    df = df.drop_duplicates()\n    print(f\"Duplicados removidos: {n_before - len(df)} | shape atual: {df.shape}\")\n\n    # 3) Missing antes\n    print_header(\"[3] Missing values (antes)\")\n    miss_before = df.isna().sum()\n    print(miss_before[miss_before &gt; 0] if miss_before.sum() &gt; 0 else \"Sem valores ausentes.\")\n\n    # 4) Normalizar categorias inv\u00e1lidas\n    print_header(\"[4] Normalizando categorias inv\u00e1lidas (education, marriage)\")\n    if \"education\" in df.columns:\n        df[\"education\"] = df[\"education\"].replace({0: 4, 5: 4, 6: 4})\n    if \"marriage\" in df.columns:\n        df[\"marriage\"] = df[\"marriage\"].replace({0: 3})\n\n    # 5) Categ\u00f3ricas vs. num\u00e9ricas\n    print_header(\"[5] Identificando colunas categ\u00f3ricas e num\u00e9ricas\")\n    all_cols = df.columns.tolist()\n    cat_cols = [c for c in CAT_COLS_RAW if c in df.columns]\n    num_cols = [c for c in all_cols if c not in cat_cols + [target_col] + ([id_col] if id_col else [])]\n    print(f\"Categ\u00f3ricas: {cat_cols}\")\n    print(f\"Num\u00e9ricas (exemplos): {num_cols[:10]} ... (total {len(num_cols)})\")\n\n    # 6) Distribui\u00e7\u00e3o do alvo\n    print_header(\"[6] Distribui\u00e7\u00e3o do alvo (antes do split)\")\n    class_dist = df[target_col].value_counts().sort_index().to_dict()\n    print(f\"Distribui\u00e7\u00e3o de classes: {class_dist}\")\n    with open(outdir / \"class_dist_before.json\", \"w\") as f:\n        json.dump({int(k): int(v) for k, v in class_dist.items()}, f, indent=2)\n\n    # 7) One-Hot\n    print_header(\"[7] One-Hot Encoding\")\n    X_cat = pd.get_dummies(df[cat_cols].astype(\"category\"), drop_first=False) if cat_cols else pd.DataFrame(index=df.index)\n    X_num = df[num_cols].copy()\n\n    print(\"Percentis num\u00e9ricos (ANTES do winsorize) [p1, p50, p99]:\")\n    for c in num_cols:\n        p1, p50, p99 = np.percentile(X_num[c], [1, 50, 99])\n        print(f\"  {c:&gt;18s}: p1={p1:.2f}, p50={p50:.2f}, p99={p99:.2f}\")\n\n    print_header(f\"[8] Winsorize/clip num\u00e9ricas (p={WINSOR_LO:.1f}\u2013{WINSOR_HI:.1f})\")\n    for c in num_cols:\n        lo, hi = np.percentile(X_num[c], [WINSOR_LO, WINSOR_HI])\n        X_num[c] = X_num[c].clip(lo, hi)\n    print(\"Percentis num\u00e9ricos (DEPOIS do winsorize) [p1, p50, p99]:\")\n    for c in num_cols:\n        p1, p50, p99 = np.percentile(X_num[c], [1, 50, 99])\n        print(f\"  {c:&gt;18s}: p1={p1:.2f}, p50={p50:.2f}, p99={p99:.2f}\")\n\n    X = pd.concat([X_num, X_cat], axis=1)\n    y = df[target_col].to_numpy().astype(int)\n\n    print_header(\"[10] Missing values (depois do encoding/winsorize)\")\n    miss_after = X.isna().sum()\n    if miss_after.sum() &gt; 0:\n        print(miss_after[miss_after &gt; 0].sort_values(ascending=False))\n        for c in X.columns:\n            if X[c].isna().any():\n                med = X[c].median() if X[c].dtype.kind in \"if\" else 0\n                X[c] = X[c].fillna(med)\n        print(\"Ap\u00f3s imputa\u00e7\u00e3o:\", int(X.isna().sum().sum()), \"missing restantes (esperado: 0).\")\n    else:\n        print(\"Sem valores ausentes ap\u00f3s transforma\u00e7\u00f5es.\")\n\n    # 11) Split 60/20/20\n    print_header(\"[11] Split estratificado 60/20/20 (train/val/test)\")\n    X_np = X.to_numpy(dtype=np.float32)\n    X_train, X_tmp, y_train, y_tmp = train_test_split(\n        X_np, y, test_size=0.4, random_state=SEED, stratify=y\n    )\n    X_val, X_test, y_val, y_test = train_test_split(\n        X_tmp, y_tmp, test_size=0.5, random_state=SEED, stratify=y_tmp\n    )\n    dist_splits = {\n        \"train\": {int(k): int(v) for k, v in pd.Series(y_train).value_counts().sort_index().to_dict().items()},\n        \"val\":   {int(k): int(v) for k, v in pd.Series(y_val).value_counts().sort_index().to_dict().items()},\n        \"test\":  {int(k): int(v) for k, v in pd.Series(y_test).value_counts().sort_index().to_dict().items()},\n    }\n    print(\"Distribui\u00e7\u00e3o por split:\", json.dumps(dist_splits, indent=2))\n    with open(outdir / \"class_dist_splits.json\", \"w\") as f:\n        json.dump(dist_splits, f, indent=2)\n\n    # 12) Z-score\n    print_header(\"[12] Z-score (fit no treino, aplicar em val/test)\")\n    mu = X_train.mean(axis=0, keepdims=True)\n    sd = X_train.std(axis=0, keepdims=True) + 1e-8\n    X_train_z = (X_train - mu) / sd\n    X_val_z   = (X_val   - mu) / sd\n    X_test_z  = (X_test  - mu) / sd\n    print(f\"M\u00e9dia m\u00e9dia (train, p\u00f3s z-score) \u2248 {X_train_z.mean():.4f} | Desvio m\u00e9dio \u2248 {X_train_z.std():.4f}\")\n\n    # 13) Salvar artefatos\n    print_header(\"[13] Salvando artefatos e datasets\")\n    np.save(outdir / \"X_train.npy\", X_train_z)\n    np.save(outdir / \"y_train.npy\", y_train)\n    np.save(outdir / \"X_val.npy\", X_val_z)\n    np.save(outdir / \"y_val.npy\", y_val)\n    np.save(outdir / \"X_test.npy\", X_test_z)\n    np.save(outdir / \"y_test.npy\", y_test)\n    np.save(outdir / \"scaler_mu.npy\", mu.astype(np.float32))\n    np.save(outdir / \"scaler_sd.npy\", sd.astype(np.float32))\n    columns = X.columns.tolist()\n    with open(outdir / \"columns.json\", \"w\") as f:\n        json.dump({\"columns\": columns}, f, indent=2)\n\n    if SAMPLE_CSV_FRAC &gt; 0:\n        print(f\"Salvando amostra limpa ({SAMPLE_CSV_FRAC*100:.1f}%)\u2026\")\n        X_all_z = np.vstack([X_train_z, X_val_z, X_test_z])\n        y_all   = np.concatenate([y_train, y_val, y_test])\n        df_clean = pd.DataFrame(X_all_z, columns=columns)\n        df_clean[\"target\"] = y_all\n        df_clean.sample(frac=SAMPLE_CSV_FRAC, random_state=SEED).to_csv(\n            outdir / \"cleaned_full_sample.csv\", index=False\n        )\n\n    print(\"\\n\u2705 Cleaning conclu\u00eddo. Artefatos salvos em:\", outdir.resolve())\n\n\n# ------------------------------------------------------------------------------\n#                     SECTION 4 \u2014 MLP IMPLEMENTATION (NumPy)\n# ------------------------------------------------------------------------------\n\n# -------- utils de m\u00e9tricas e batches --------\ndef one_hot(y, n_classes):\n    oh = np.zeros((y.shape[0], n_classes), dtype=np.float32)\n    oh[np.arange(y.shape[0]), y] = 1.0\n    return oh\n\ndef accuracy(y_true, y_pred):\n    return float((y_true == y_pred).mean())\n\ndef precision_recall_f1(y_true, y_pred, positive=1):\n    tp = np.sum((y_true == positive) &amp; (y_pred == positive))\n    fp = np.sum((y_true != positive) &amp; (y_pred == positive))\n    fn = np.sum((y_true == positive) &amp; (y_pred != positive))\n    prec = tp / (tp + fp + 1e-12)\n    rec  = tp / (tp + fn + 1e-12)\n    f1   = 2*prec*rec/(prec+rec+1e-12)\n    return float(prec), float(rec), float(f1)\n\ndef roc_auc_score_binary(y_true, scores):\n    pos = scores[y_true == 1]\n    neg = scores[y_true == 0]\n    if len(pos) == 0 or len(neg) == 0:\n        return float(\"nan\")\n    concat = np.concatenate([pos, neg])\n    order = np.argsort(concat, kind=\"mergesort\")\n    ranks = np.empty_like(order, dtype=np.float64)\n    ranks[order] = np.arange(1, len(concat) + 1)  # 1..N\n    r_pos = ranks[:len(pos)]\n    auc = (r_pos.sum() - len(pos)*(len(pos)+1)/2) / (len(pos)*len(neg) + 1e-12)\n    return float(auc)\n\ndef iterate_minibatches(X, Y, batch, rng):\n    idx = rng.permutation(len(X))\n    for i in range(0, len(X), batch):\n        ib = idx[i:i+batch]\n        yield X[ib], Y[ib]\n\n\n# -------- classe MLP --------\nclass MLP:\n    def __init__(self, d_in, layers=(64,), d_out=2, seed=42, l2=1e-4, momentum=0.9):\n        rng = np.random.default_rng(seed)\n        self.l2 = l2\n        self.class_weights = None  # definido externamente, se desejado\n        self.momentum = momentum\n        dims = [d_in] + list(layers) + [d_out]\n        self.params = {}\n        self.v = {}  # velocidades para momentum\n\n        # Inicializa\u00e7\u00e3o He (ReLU): N(0, sqrt(2/fan_in))\n        for i in range(len(dims)-1):\n            fan_in = dims[i]\n            W = rng.normal(0, np.sqrt(2.0/fan_in), size=(dims[i], dims[i+1])).astype(np.float32)\n            b = np.zeros((1, dims[i+1]), dtype=np.float32)\n            self.params[f\"W{i+1}\"] = W\n            self.params[f\"b{i+1}\"] = b\n            self.v[f\"W{i+1}\"] = np.zeros_like(W)\n            self.v[f\"b{i+1}\"] = np.zeros_like(b)\n\n    def set_class_weights(self, cw):\n        self.class_weights = np.asarray(cw, dtype=np.float32)\n\n    def init_output_bias_with_prior(self, p_pos):\n        \"\"\"Define b0=0 e b1=logit(p) para sa\u00edda bin\u00e1ria.\"\"\"\n        p = float(np.clip(p_pos, 1e-6, 1-1e-6))\n        logit = np.log(p / (1.0 - p)).astype(np.float32)\n        L = len(self.params)//2\n        b = self.params[f\"b{L}\"].copy()\n        if b.shape[1] == 2:\n            b[:, 0] = 0.0\n            b[:, 1] = logit\n            self.params[f\"b{L}\"] = b\n\n    @staticmethod\n    def relu(x):\n        return np.maximum(0, x)\n\n    @staticmethod\n    def relu_grad(x):\n        return (x &gt; 0).astype(np.float32)\n\n    @staticmethod\n    def softmax(z):\n        z = z - z.max(axis=1, keepdims=True)\n        e = np.exp(z, dtype=np.float32)\n        return e / (e.sum(axis=1, keepdims=True) + 1e-12)\n\n    def forward(self, X):\n        cache = {\"A0\": X}\n        A = X\n        L = len(self.params)//2\n        for i in range(1, L):\n            Z = A @ self.params[f\"W{i}\"] + self.params[f\"b{i}\"]\n            A = self.relu(Z)\n            cache[f\"Z{i}\"] = Z; cache[f\"A{i}\"] = A\n        ZL = A @ self.params[f\"W{L}\"] + self.params[f\"b{L}\"]\n        P = self.softmax(ZL)\n        cache[f\"Z{L}\"] = ZL; cache[f\"A{L}\"] = P\n        return P, cache\n\n    def loss(self, P, Y_onehot):\n        # Cross-Entropy ponderada por classe + L2\n        if self.class_weights is None:\n            cw = np.ones(Y_onehot.shape[1], dtype=np.float32)\n        else:\n            cw = self.class_weights\n        w_i = (Y_onehot * cw).sum(axis=1)  # peso por amostra\n        sum_w = float(w_i.sum() + 1e-12)\n        ce_per_sample = -np.sum(Y_onehot * np.log(P + 1e-12), axis=1)\n        ce = float(np.sum(w_i * ce_per_sample) / sum_w)\n\n        l2_term = 0.0\n        L = len(self.params)//2\n        for i in range(1, L+1):\n            l2_term += np.sum(self.params[f\"W{i}\"]**2)\n        return ce + self.l2 * 0.5 * l2_term\n\n    def backward(self, cache, Y_onehot):\n        grads = {}\n        if self.class_weights is None:\n            cw = np.ones(Y_onehot.shape[1], dtype=np.float32)\n        else:\n            cw = self.class_weights\n        w_i = (Y_onehot * cw).sum(axis=1)[:, None]  # (N,1)\n        sum_w = float(w_i.sum() + 1e-12)\n\n        L = len(self.params)//2\n        A_L = cache[f\"A{L}\"]  # probs\n\n        # dZ (softmax + CE) ponderado\n        dZ = ((A_L - Y_onehot) * w_i) / sum_w\n        A_prev = cache[f\"A{L-1}\"] if L &gt; 1 else cache[\"A0\"]\n        grads[f\"W{L}\"] = A_prev.T @ dZ + self.l2 * self.params[f\"W{L}\"]\n        grads[f\"b{L}\"] = np.sum(dZ, axis=0, keepdims=True)\n        dA_prev = dZ @ self.params[f\"W{L}\"].T\n\n        # camadas escondidas (ReLU)\n        for i in range(L-1, 0, -1):\n            Z = cache[f\"Z{i}\"]; A_prev = cache[f\"A{i-1}\"] if i &gt; 1 else cache[\"A0\"]\n            dZ = dA_prev * self.relu_grad(Z)\n            grads[f\"W{i}\"] = A_prev.T @ dZ + self.l2 * self.params[f\"W{i}\"]\n            grads[f\"b{i}\"] = np.sum(dZ, axis=0, keepdims=True)\n            dA_prev = dZ @ self.params[f\"W{i}\"].T\n        return grads\n\n    def step_sgd(self, grads, lr):\n        \"\"\"SGD com momentum cl\u00e1ssico.\"\"\"\n        L = len(self.params)//2\n        for i in range(1, L+1):\n            self.v[f\"W{i}\"] = self.momentum * self.v[f\"W{i}\"] + grads[f\"W{i}\"]\n            self.v[f\"b{i}\"] = self.momentum * self.v[f\"b{i}\"] + grads[f\"b{i}\"]\n            self.params[f\"W{i}\"] -= lr * self.v[f\"W{i}\"]\n            self.params[f\"b{i}\"] -= lr * self.v[f\"b{i}\"]\n\n    def predict_proba(self, X):\n        P, _ = self.forward(X)\n        return P\n\n    def predict(self, X, threshold=None):\n        P, _ = self.forward(X)\n        if threshold is None:\n            return np.argmax(P, axis=1), P[:, 1]\n        else:\n            ppos = P[:, 1]\n            yhat = (ppos &gt;= threshold).astype(int)\n            return yhat, ppos\n\n\ndef train_mlp_numpy(outdir: Path):\n    print_header(\"\ud83d\udd27 [4] MLP Implementation (NumPy) \u2014 Treino/Val/Test\")\n\n    # carregar dados processados\n    X_train = np.load(outdir / \"X_train.npy\")\n    y_train = np.load(outdir / \"y_train.npy\")\n    X_val   = np.load(outdir / \"X_val.npy\")\n    y_val   = np.load(outdir / \"y_val.npy\")\n    X_test  = np.load(outdir / \"X_test.npy\")\n    y_test  = np.load(outdir / \"y_test.npy\")\n\n    n_classes = int(np.max([y_train.max(), y_val.max(), y_test.max()]) + 1)\n    Y_train = one_hot(y_train, n_classes)\n    Y_val   = one_hot(y_val, n_classes)\n\n    # class weights \"balanced\": N / (K * n_c)\n    counts = np.bincount(y_train, minlength=n_classes).astype(np.float32)\n    cw_balanced = (len(y_train) / (n_classes * counts + 1e-12)).astype(np.float32)\n\n    mlp = MLP(\n        d_in=X_train.shape[1],\n        layers=MLP_LAYERS,\n        d_out=n_classes,\n        seed=SEED,\n        l2=MLP_L2,\n        momentum=MOMENTUM\n    )\n    mlp.set_class_weights(cw_balanced)\n\n    # inicializa vi\u00e9s de sa\u00edda com a preval\u00eancia da classe positiva (para bin\u00e1rio)\n    p_pos_train = float((y_train == 1).mean())\n    mlp.init_output_bias_with_prior(p_pos_train)\n\n    rng = np.random.default_rng(SEED)\n    lr = LR_INIT\n    best_vl = np.inf\n    wait = 0\n    hist = {\"loss_tr\": [], \"loss_vl\": []}\n    best_params = {k: v.copy() for k, v in mlp.params.items()}\n\n    for ep in range(1, EPOCHS+1):\n        # ===== treino (mini-batch SGD cobrindo TODO o dataset) =====\n        for xb, yb in iterate_minibatches(X_train, Y_train, BATCH_SIZE, rng):\n            P, cache = mlp.forward(xb)\n            loss = mlp.loss(P, yb)\n            grads = mlp.backward(cache, yb)\n            mlp.step_sgd(grads, lr)\n\n        # logging perdas em fim de \u00e9poca\n        P_tr = mlp.predict_proba(X_train); loss_tr = mlp.loss(P_tr, Y_train)\n        P_vl = mlp.predict_proba(X_val);   loss_vl = mlp.loss(P_vl, Y_val)\n        hist[\"loss_tr\"].append(loss_tr); hist[\"loss_vl\"].append(loss_vl)\n        print(f\"Epoch {ep:02d} | loss_tr={loss_tr:.4f}  loss_vl={loss_vl:.4f}  lr={lr:.4f}\")\n\n        # early stopping\n        if loss_vl &lt; best_vl - 1e-4:\n            best_vl = loss_vl; wait = 0\n            best_params = {k: v.copy() for k, v in mlp.params.items()}\n        else:\n            wait += 1\n            if wait &gt;= PATIENCE:\n                print(\"Early stopping acionado.\")\n                break\n\n        # lr decay\n        if (ep % LR_DECAY_EVERY) == 0:\n            lr *= LR_DECAY\n\n    # restaurar melhores pesos\n    mlp.params = best_params\n\n    # ---- escolher threshold pelo melhor F1 na valida\u00e7\u00e3o ----\n    _, ppos_val = mlp.predict(X_val, threshold=None)\n    best_t, best_f1 = 0.5, -1.0\n    for t in np.linspace(0.05, 0.95, 91):\n        yhat_t = (ppos_val &gt;= t).astype(int)\n        _, _, f1_t = precision_recall_f1(y_val, yhat_t, positive=1)\n        if f1_t &gt; best_f1:\n            best_f1 = f1_t\n            best_t = float(t)\n    print(f\"\\nThreshold escolhido na valida\u00e7\u00e3o (melhor F1): t* = {best_t:.2f} (F1={best_f1:.4f})\")\n    print(f\"Propor\u00e7\u00e3o prevista como positiva em val @t*: {(ppos_val &gt;= best_t).mean():.3f}\")\n\n    # avalia\u00e7\u00e3o\n    def report(split, X, y, threshold):\n        y_pred, ppos = mlp.predict(X, threshold=threshold)\n        acc = accuracy(y, y_pred)\n        prec, rec, f1 = precision_recall_f1(y, y_pred, positive=1)\n        auc = roc_auc_score_binary(y, ppos)\n        print(f\"[{split}] acc={acc:.4f}  prec={prec:.4f}  rec={rec:.4f}  f1={f1:.4f}  auc={auc:.4f}\")\n        return {\"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1, \"auc\": auc, \"pos_rate\": float((y_pred==1).mean())}\n\n    print()\n    mtr_tr = report(\"train\", X_train, y_train, threshold=best_t)\n    mtr_vl = report(\"val\",   X_val,   y_val,   threshold=best_t)\n    mtr_te = report(\"test\",  X_test,  y_test,  threshold=best_t)\n\n    # salvar curvas\n    pd.DataFrame(hist).to_csv(outdir / \"training_curves.csv\", index=False)\n    # salvar m\u00e9tricas finais + threshold\n    final_metrics = {\n        \"train\": mtr_tr, \"val\": mtr_vl, \"test\": mtr_te,\n        \"layers\": list(MLP_LAYERS), \"l2\": MLP_L2, \"lr_init\": LR_INIT,\n        \"batch_size\": BATCH_SIZE, \"epochs\": EPOCHS, \"patience\": PATIENCE,\n        \"class_weights\": cw_balanced.tolist(),\n        \"threshold_val_f1\": best_t,\n        \"momentum\": MOMENTUM\n    }\n    with open(outdir / \"final_metrics.json\", \"w\") as f:\n        json.dump(final_metrics, f, indent=2)\n\n    print(\"\\n\u2705 Treinamento conclu\u00eddo. M\u00e9tricas salvas em:\", outdir / \"final_metrics.json\")\n    print(\"Curvas de treino salvas em:\", outdir / \"training_curves.csv\")\n\n\n# ------------------------------------------------------------------------------\n#                                      MAIN\n# ------------------------------------------------------------------------------\n\ndef main():\n    script_dir = Path(__file__).parent\n    outdir = script_dir / OUTDIR_NAME\n\n    # Se ainda n\u00e3o existir X_train.npy, roda o cleaning\n    need_clean = not (outdir / \"X_train.npy\").exists()\n    if need_clean:\n        run_cleaning_and_save(script_dir)\n    else:\n        print_header(\"\ud83d\udd01 Artefatos de cleaning encontrados \u2014 pulando etapa de limpeza.\")\n\n    # Treinar MLP (NumPy)\n    train_mlp_numpy(outdir)\n\n\nif __name__ == \"__main__\":\n    main()\n\n</pre> Output Clear <pre></pre> </p>"},{"location":"projeto%201/main/#5-curvas-de-erro-e-visualizacoes","title":"5. Curvas de Erro e Visualiza\u00e7\u00f5es","text":"<p>A curva de perda (loss) foi monitorada durante o treinamento do MLP, tanto no conjunto de treino quanto de valida\u00e7\u00e3o, ao longo das \u00e9pocas. Isso permite avaliar o comportamento do modelo em rela\u00e7\u00e3o a converg\u00eancia, overfitting e early stopping.</p> <p></p> <p>Como mostrado no gr\u00e1fico acima:</p> <ul> <li>O loss de treino decresce consistentemente at\u00e9 estabilizar.</li> <li>O loss de valida\u00e7\u00e3o tamb\u00e9m decresce nas primeiras \u00e9pocas, mas apresenta uma estabiliza\u00e7\u00e3o e flutua\u00e7\u00e3o posterior.</li> <li>O modelo utilizou early stopping com paci\u00eancia de 8 \u00e9pocas, interrompendo o treinamento antes de overfitting.</li> <li>A partir da \u00e9poca ~20, n\u00e3o houve mais ganhos relevantes na valida\u00e7\u00e3o, indicando que o modelo j\u00e1 havia convergido.</li> </ul> <p>Esse padr\u00e3o \u00e9 t\u00edpico em dados com certo desbalanceamento: o modelo consegue otimizar a perda, mas o ganho em recall e F1 tende a saturar cedo.</p>"},{"location":"projeto%201/main/#codigo-para-gerar-a-curva-de-perda","title":"C\u00f3digo para gerar a curva de perda:","text":"<p> Editor (session: default) Run <pre>\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Garantir que o diret\u00f3rio de sa\u00edda existe\nos.makedirs(\"assets\", exist_ok=True)\n\n# Carregar o CSV com as curvas\ndf = pd.read_csv(\"processed/training_curves.csv\")\n\n# Plotar curvas de perda\nplt.figure(figsize=(8, 5))\nplt.plot(df[\"loss_tr\"], label=\"Treino\", linewidth=2)\nplt.plot(df[\"loss_vl\"], label=\"Valida\u00e7\u00e3o\", linewidth=2)\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Loss (Cross-Entropy)\")\nplt.title(\"Curva de perda (loss) por \u00e9poca\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\n\n# Salvar imagem\nplt.savefig(\"assets/loss_curve.png\")\nplt.show()\n</pre> Output Clear <pre></pre> </p>"},{"location":"projeto%201/main/#6-metricas-de-avaliacao-e-matriz-de-confusao","title":"6. M\u00e9tricas de Avalia\u00e7\u00e3o e Matriz de Confus\u00e3o","text":"<p>Al\u00e9m das m\u00e9tricas quantitativas (acur\u00e1cia, precis\u00e3o, recall, F1-score e AUC), a matriz de confus\u00e3o oferece uma vis\u00e3o clara dos tipos de erro que o modelo comete ao classificar clientes no conjunto de teste.</p> <p></p>"},{"location":"projeto%201/main/#codigo-para-gerar-a-matriz-de-confusao","title":"C\u00f3digo para gerar a matriz de confus\u00e3o:","text":"<p> Editor (session: default) Run <pre>\nimport numpy as np\nimport json\nfrom pathlib import Path\nfrom projeto1 import MLP, one_hot  # importa sua classe e fun\u00e7\u00e3o\n\n# Paths\noutdir = Path(\"processed\")\nX_test = np.load(outdir / \"X_test.npy\")\ny_test = np.load(outdir / \"y_test.npy\")\nX_train = np.load(outdir / \"X_train.npy\")\ny_train = np.load(outdir / \"y_train.npy\")\nX_val = np.load(outdir / \"X_val.npy\")\ny_val = np.load(outdir / \"y_val.npy\")\n\n# Hiperpar\u00e2metros (voc\u00ea pode pegar isso do final_metrics.json tamb\u00e9m)\nwith open(outdir / \"final_metrics.json\") as f:\n    final_metrics = json.load(f)\n\nlayers = tuple(final_metrics[\"layers\"])\nl2 = final_metrics[\"l2\"]\nmomentum = final_metrics[\"momentum\"]\nthreshold = final_metrics[\"threshold_val_f1\"]\nbatch_size = final_metrics[\"batch_size\"]\nepochs = final_metrics[\"epochs\"]\nlr_init = final_metrics[\"lr_init\"]\npatience = final_metrics[\"patience\"]\nclass_weights = np.array(final_metrics[\"class_weights\"], dtype=np.float32)\n\n# One-hot\nY_train = one_hot(y_train, 2)\nY_val = one_hot(y_val, 2)\n\n# Treinar novamente o MLP com os mesmos dados (replicando treino para recuperar pesos)\nmlp = MLP(\n    d_in=X_train.shape[1],\n    layers=layers,\n    d_out=2,\n    seed=42,\n    l2=l2,\n    momentum=momentum\n)\nmlp.set_class_weights(class_weights)\nmlp.init_output_bias_with_prior(float((y_train == 1).mean()))\n\n# Repetir treino\nrng = np.random.default_rng(42)\nbest_params = {k: v.copy() for k, v in mlp.params.items()}\nbest_vl = float(\"inf\")\nwait = 0\nlr = lr_init\n\ndef iterate_minibatches(X, Y, batch, rng):\n    idx = rng.permutation(len(X))\n    for i in range(0, len(X), batch):\n        ib = idx[i:i+batch]\n        yield X[ib], Y[ib]\n\nfor ep in range(1, epochs+1):\n    for xb, yb in iterate_minibatches(X_train, Y_train, batch_size, rng):\n        P, cache = mlp.forward(xb)\n        loss = mlp.loss(P, yb)\n        grads = mlp.backward(cache, yb)\n        mlp.step_sgd(grads, lr)\n    # Valida\u00e7\u00e3o\n    P_vl, _ = mlp.forward(X_val)\n    loss_vl = mlp.loss(P_vl, Y_val)\n    if loss_vl &lt; best_vl - 1e-4:\n        best_vl = loss_vl\n        wait = 0\n        best_params = {k: v.copy() for k, v in mlp.params.items()}\n    else:\n        wait += 1\n        if wait &gt;= patience:\n            break\n    if ep % 5 == 0:\n        lr *= 0.9\n\n# Restaurar melhores pesos\nmlp.params = best_params\n\n# Predi\u00e7\u00e3o no teste com threshold \u00f3timo\nyhat_test, _ = mlp.predict(X_test, threshold=threshold)\nnp.save(outdir / \"final_yhat_test.npy\", yhat_test)\nprint(\"\u2705 Predi\u00e7\u00f5es salvas em processed/final_yhat_test.npy\")\n\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Carregar dados\ny_test = np.load(\"processed/y_test.npy\")\ny_pred = np.load(\"processed/final_yhat_test.npy\")  # ou gere a predi\u00e7\u00e3o no seu script final\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nlabels = [\"N\u00e3o Inadimplente\", \"Inadimplente\"]\n\n# Plot\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.xlabel(\"Predito\")\nplt.ylabel(\"Real\")\nplt.title(\"Matriz de Confus\u00e3o \u2014 Conjunto de Teste\")\nplt.tight_layout()\nplt.savefig(\"assets/confusion_matrix.png\")  # ajuste o caminho conforme o GitHub Pages\nplt.show()\n</pre> Output Clear <pre></pre> </p>"},{"location":"projeto%201/main/#interpretacao","title":"Interpreta\u00e7\u00e3o:","text":"<ul> <li>3596 clientes foram corretamente identificados como n\u00e3o inadimplentes (verdadeiros negativos).</li> <li>855 clientes inadimplentes foram corretamente identificados (verdadeiros positivos).</li> <li>472 inadimplentes foram classificados como n\u00e3o inadimplentes (falsos negativos), o que representa um risco de cr\u00e9dito n\u00e3o detectado.</li> <li>1077 n\u00e3o inadimplentes foram classificados incorretamente como inadimplentes (falsos positivos), o que pode levar \u00e0 recusa de cr\u00e9dito injusta.</li> </ul>"},{"location":"projeto%201/main/#conclusao","title":"Conclus\u00e3o:","text":"<p>O modelo apresenta um bom desempenho em recuperar inadimplentes, com 855 acertos, mas ainda comete 472 erros cr\u00edticos (falsos negativos) \u2014 o que pode impactar negativamente institui\u00e7\u00f5es financeiras que dependem dessa previs\u00e3o para concess\u00e3o de cr\u00e9dito.</p> <p>A calibragem via threshold \u00f3timo na valida\u00e7\u00e3o priorizou o F1-score e recall da classe minorit\u00e1ria, aceitando sacrificar parte da precis\u00e3o para detectar mais inadimplentes. Essa escolha foi intencional, considerando que o custo de um falso negativo (inadimplente n\u00e3o detectado) \u00e9 normalmente maior que o de um falso positivo.</p> <p>Nota: Os dados s\u00e3o desbalanceados (~22% inadimplentes), e por isso o modelo foi treinado com pesos de classe ajustados, regulariza\u00e7\u00e3o L2, e early stopping, al\u00e9m da normaliza\u00e7\u00e3o por z-score.</p>"},{"location":"projeto%201/main/#uso-de-ia","title":"Uso de I.A.","text":"<p>Utilizamos o aux\u00edlio do chatGPT para: - Fazer README do projeto. - Gerar fun\u00e7\u00f5es auxiliares em python. - Revisar e melhorar trechos de c\u00f3digo.</p>"},{"location":"roteiro1/main/","title":"Roteiro - Data","text":"<p>Este relat\u00f3rio cont\u00e9m: gera\u00e7\u00e3o dos dados (Ex.1 e Ex.2), visualiza\u00e7\u00f5es e pr\u00e9-processamento do Spaceship Titanic (Ex.3), com an\u00e1lise explicativa.</p>"},{"location":"roteiro1/main/#visao-geral-do-pipeline","title":"Vis\u00e3o geral do pipeline","text":"<pre><code>flowchart LR\n  A[Ex.1: Dados 2D Gauss] --&gt; B[Visualiza\u00e7\u00e3o &amp; Fronteiras]\n  C[Ex.2: Dados 5D MVN] --&gt; D[PCA 5\u21922 e Scatter]\n  E[Ex.3: Kaggle Titanic] --&gt; F[Limpeza &amp; Escala tanh]\n  B --&gt; G[An\u00e1lises]\n  D --&gt; G\n  F --&gt; G</code></pre>"},{"location":"roteiro1/main/#exercicio-1","title":"Exerc\u00edcio 1","text":"<p>Gera\u00e7\u00e3o e plotagem dos dados (o output do md exec n\u00e3o funciona com plt.show, ent\u00e3o o gr\u00e1fico da sa\u00edda \u00e9 mostrado abaixo do c\u00f3digo como imagem):   </p> <p>obs: ajuda de IA</p> <p> Editor (session: default) Run <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(42)\n\n# Par\u00e2metros fornecidos\nmeans = {\n    0: np.array([2.0, 3.0]),\n    1: np.array([5.0, 6.0]),\n    2: np.array([8.0, 1.0]),\n    3: np.array([15.0, 14.0]),\n}\nstds = {\n    0: np.array([0.8, 2.5]),\n    1: np.array([1.2, 1.9]),\n    2: np.array([0.9, 0.9]),\n    3: np.array([0.5, 2.0]),\n}\n\nX_list, y_list = [], []\nfor k in range(4):\n    mu = means[k]\n    sd = stds[k]\n    cov = np.diag(sd**2)\n    Xi = rng.multivariate_normal(mu, cov, size=100)\n    yi = np.full(100, k)\n    X_list.append(Xi)\n    y_list.append(yi)\n\nX = np.vstack(X_list)\ny = np.concatenate(y_list)\n\n# Scatter por classe\nfig, ax = plt.subplots(figsize=(6,5))\nfor k, color in zip(range(4), [\"tab:blue\",\"tab:orange\",\"tab:green\",\"tab:red\"]):\n    ax.scatter(X[y==k,0], X[y==k,1], s=18, alpha=0.85, label=f\"Classe {k}\", c=color)\n\nax.set_title(\"Ex.1 \u2014 Dados 2D (4 classes, gaussianas)\")\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nax.legend(loc=\"best\", frameon=True)\nax.grid(True, ls=\":\")\nplt.show()\n</pre> Output Clear <pre></pre> </p> <p></p>"},{"location":"roteiro1/main/#analyze-and-draw-boundaries","title":"Analyze and Draw Boundaries:","text":"<ul> <li>Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.</li> </ul> <p>Red is isolated in the top right, green is isolated in the bottom right (but close to blue and orange clusters), blue is mostly on the left side with some spread, and orange is in the middle overlapping a little bit with blue.</p> <ul> <li>Based on your visual inspection, could a simple, linear boundary separate all classes?</li> </ul> <p>No, because it is impossible to separate class 0 and 1 using a line. This means that a linear model would not be able to classify these points correctly.</p> <ul> <li>On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</li> </ul> <p>It should probably look something like this:</p> <p></p>"},{"location":"roteiro1/main/#exercicio-2","title":"Exerc\u00edcio 2","text":"<p>Generating the Data</p> <p> Editor (session: default) Run <pre>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(123)\n\nmiA = np.zeros(5)\nSomA = np.array([\n    [1,   0.8, 0.1, 0,   0  ],\n    [0.8, 1,   0.3, 0,   0  ],\n    [0.1, 0.3, 1,   0.5, 0  ],\n    [0,   0,   0.5, 1,   0.2],\n    [0,   0,   0,   0.2, 1  ],\n], dtype=float)\n\nmiB = np.full(5, 1.5)\nSomB = np.array([\n    [1.5, -0.7, 0.2, 0,   0  ],\n    [-0.7, 1.5, 0.4, 0,   0  ],\n    [0.2,  0.4, 1.5, 0.6, 0  ],\n    [0,    0,   0.6, 1.5, 0.3],\n    [0,    0,   0,   0.3, 1.5],\n], dtype=float)\n\ndef nearest_psd(M, eps=1e-8):\n    # Corrige PSD numericamente (clipe de autovalores)\n    w, v = np.linalg.eigh(M)\n    w = np.clip(w, eps, None)\n    return (v * w) @ v.T\n\nSomA = nearest_psd(SomA)\nSomB = nearest_psd(SomB)\n\nXA = rng.multivariate_normal(miA, SomA, size=500)\nXB = rng.multivariate_normal(miB, SomB, size=500)\n\nX5 = np.vstack([XA, XB])\ny5 = np.array([0]*500 + [1]*500)\n\n# PCA via SVD\nXc = X5 - X5.mean(axis=0, keepdims=True)\nU, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n# Componentes principais nas colunas de V (linhas de Vt)\nW = Vt[:2].T            # 5x2\nX2 = Xc @ W             # proje\u00e7\u00e3o 2D\n\n# vari\u00e2ncia explicada\nexplained = (S**2) / (len(X5)-1)\nratio = explained / explained.sum()\nvar_ratio = ratio[:2]\n\nfig, ax = plt.subplots(figsize=(6,5))\nax.scatter(X2[y5==0,0], X2[y5==0,1], s=14, alpha=0.7, label=\"Classe A\", c=\"tab:blue\")\nax.scatter(X2[y5==1,0], X2[y5==1,1], s=14, alpha=0.7, label=\"Classe B\", c=\"tab:red\")\nax.set_title(f\"Ex.2 \u2014 PCA (5D\u21922D), vari\u00e2ncia explicada PC1+PC2 = {var_ratio.sum():.3f}\")\nax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\")\nax.legend(frameon=True); ax.grid(True, ls=\":\")\nplt.show()\n</pre> Output Clear <pre></pre> </p> <p></p>"},{"location":"roteiro1/main/#analyzing-the-plots","title":"Analyzing the Plots:","text":"<ul> <li>Based on your 2D projection, describe the relationship between the two classes.</li> </ul> <p>The two classes are heavily overlapping in the 2D projection, therefore it would be impossible to separate them with high accuracy using only two dimension.</p> <ul> <li>Discuss the linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.</li> </ul> <p>The data is not linearly separable in the 2D projection, as there is significant overlap between the two classes. This means that no straight line can be drawn to separate all points of one class from the other. Simple linear models, such as logistic regression or linear SVMs, rely on finding a linear decision boundary to classify data points. Therefore, a multi-layer neural network with non-linear activation functions is necessary to capture the complex relationships and patterns in the data, allowing for more flexible decision boundaries that can better separate the classes.</p>"},{"location":"roteiro1/main/#exercicio-3","title":"Exerc\u00edcio 3","text":""},{"location":"roteiro1/main/#describing-the-data","title":"Describing the Data:","text":"<p>Briefly describe the dataset's objective (i.e., what does the Transported column represent?).</p> <p>O objetivo \u00e9 prever a coluna Transported, que indica se o passageiro foi transportado para outra dimens\u00e3o (True ou False).</p> <p>List the features and identify which are numerical (e.g., Age, RoomService) and which are categorical (e.g., HomePlanet, Destination).</p> <p>Features num\u00e9ricas:</p> <ul> <li>Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck.</li> </ul> <p>Features categ\u00f3ricas:</p> <ul> <li>HomePlanet, CryoSleep, Cabin (deck/num/side), Destination, VIP, Name.</li> </ul> <p>Identificadores:</p> <ul> <li>PassengerId (n\u00e3o usado como feature preditiva).</li> </ul> <p>Investigate the dataset for missing values. Which columns have them, and how many?</p> <p>Rodando o c\u00f3digo abaixo (n\u00e3o ir\u00e1 funcionar aqui com o markdown exec, mas funciona no Python localmente):</p> <p> Editor (session: default) Run <pre>import pandas as pd\n\n# carregar dados\nimport pandas as pd\nurl = \"https://yuritaba.github.io/personal_page/roteiro1/spaceship-titanic/train.csv\"\ndf = pd.read_csv(url)\nna_counts = df.isna().sum().sort_values(ascending=False)\nna_counts.head(15)</pre> Output Clear <pre></pre> </p> <p>Output:</p> <pre><code>CryoSleep       217\nShoppingMall    208\nVIP             203\nHomePlanet      201\nName            200\nCabin           199\nVRDeck          188\nFoodCourt       183\nSpa             183\nDestination     182\nRoomService     181\nAge             179\nPassengerId       0\nTransported       0\ndtype: int64\n</code></pre> <p>Logo, as colunas com valores ausentes s\u00e3o todas exceto PassengerId e Transported.</p> <p>Preprocess the Data: Your goal is to clean and transform the data so it can be fed into a neural network. The tanh activation function produces outputs in the range [-1, 1], so your input data should be scaled appropriately for stable training.</p> <ul> <li> <p>Encode Categorical Features: Convert categorical columns like HomePlanet, CryoSleep, and Destination into a numerical format. One-hot encoding is a good choice.</p> </li> <li> <p>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., Age, RoomService, etc.). Since the tanh activation function is centered at zero and outputs values in [-1, 1], Standardization (to mean 0, std 1) or Normalization to a [-1, 1] range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.</p> </li> </ul> <p>Primeiramente fazemos um one-hot encoding das colunas categ\u00f3ricas, depois lidamos com os valores ausentes (imputa\u00e7\u00e3o), e tamb\u00e9m normalizamos as colunas num\u00e9ricas para o intervalo [-1, 1].</p> <p>(Novamente, o c\u00f3digo abaixo n\u00e3o ir\u00e1 funcionar aqui com o markdown exec, mas funciona no Python localmente):</p> <p>obs: ajuda de IA</p> <p> Editor (session: default) Run <pre>\n# separar target\ny = df[\"Transported\"].astype(int)\nX = df.drop(columns=[\"Transported\",\"PassengerId\",\"Name\"])\n\n# separar colunas categ\u00f3ricas e num\u00e9ricas\nnum_cols = [\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"]\ncat_cols = [\"HomePlanet\",\"CryoSleep\",\"Cabin\",\"Destination\",\"VIP\"]\n\n# one-hot encoding manual\nX_cat = []\nfor col in cat_cols:\n    uniques = sorted(X[col].dropna().unique())\n    mapping = {val:i for i,val in enumerate(uniques)}\n    arr = np.zeros((len(X), len(uniques)))\n    for i, val in enumerate(X[col]):\n        if pd.isna(val):\n            continue\n        arr[i, mapping[val]] = 1\n    X_cat.append(arr)\nX_cat = np.concatenate(X_cat, axis=1) if X_cat else np.empty((len(X),0))\n\n# imputar valores num\u00e9ricos (mediana)\nfor col in num_cols:\n    median = X[col].median()\n    X[col] = X[col].fillna(median)\n\n# normalizar num\u00e9ricas para [-1,1]\nX_num = X[num_cols].values.astype(float)\nmins = X_num.min(axis=0)\nmaxs = X_num.max(axis=0)\nX_num_scaled = 2 * (X_num - mins) / (maxs - mins) - 1\n\n# juntar num\u00e9ricas e categ\u00f3ricas\nX_proc = np.concatenate([X_num_scaled, X_cat], axis=1)\n\ndf.head()</pre> Output Clear <pre><code></code></pre> </p> <ul> <li>Num\u00e9ricas \u2192 preenchidas com a mediana de cada coluna.</li> </ul> <p>A mediana \u00e9 robusta contra outliers, evitando distor\u00e7\u00f5es no escalonamento para [-1,1].</p> <ul> <li>Categ\u00f3ricas \u2192 no one-hot, valores nulos n\u00e3o viraram nenhuma categoria ativa (linha de zeros).</li> </ul> <p>Isso equivale a tratar o missing como uma \u201ccategoria ausente\u201d sem inventar valores, mantendo consist\u00eancia no vetor de entrada.</p> <p>Visualize the Results:</p> <p>Create histograms for one or two numerical features (like FoodCourt or Age) before and after scaling to show the effect of your transformation.</p> <p></p> <p>Como esperado, a \"propor\u00e7\u00e3o\" continua a mesma, mas agora os valores est\u00e3o entre -1 e 1, adequados para a tanh.</p>"},{"location":"roteiro2/main/","title":"2. Perceptron","text":""},{"location":"roteiro2/main/#diagrama-de-classes-do-banco","title":"Diagrama de Classes do Banco","text":"<pre><code>classDiagram\n    class Conta {\n        - String id\n        # double saldo\n        - Cliente cliente\n        + sacar(double valor)\n        + depositar(double valor)\n    }\n    class Cliente {\n        - String id\n        - String nome\n        - List&lt;Conta&gt; contas\n    }\n    class PessoaFisica {\n        - String cpf\n    }\n    class PessoaJuridica {\n        - String cnpj\n    }\n    class ContaCorrente {\n        - double limite\n        + sacar(double valor)\n    }\n    class ContaPoupanca {\n        + sacar(double valor)\n    }\n    Conta *-- Cliente\n    Conta &lt;|-- ContaCorrente\n    Conta &lt;|-- ContaPoupanca\n    Cliente &lt;|-- PessoaFisica\n    Cliente &lt;|-- PessoaJuridica</code></pre>"},{"location":"roteiro2/main/#diagrama-de-sequencia-de-autorizacao","title":"Diagrama de Seq\u00fc\u00eancia de Autoriza\u00e7\u00e3o","text":"<pre><code>sequenceDiagram\n  autonumber\n  actor User\n  User-&gt;&gt;Auth Service: request with token\n  Auth Service-&gt;&gt;Auth Service: decodes the token and extracts claims\n  Auth Service-&gt;&gt;Auth Service: verifies permissions\n  critical allowed\n    Auth Service-&gt;&gt;Secured Resource: authorizes the request\n    Secured Resource-&gt;&gt;User: returns the response\n  option denied\n    Auth Service--&gt;&gt;User: unauthorized message\n  end  </code></pre>"},{"location":"roteiro3/main/","title":"3. MLP","text":"<p>Running the code below in Browser (Woooooowwwwww!!!!!!). <sup>1</sup></p> <p> Editor (session: default) Run <pre>import ssl\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf['AAPL'] = pd.Series([1, 2, 3])\ndf['MSFT'] = pd.Series([4, 5, 6])\ndf['GOOGL'] = pd.Series([7, 8, 9])\n\nprint(df)\n</pre> Output Clear <pre></pre> </p> <ol> <li> <p>Pyodide \u21a9</p> </li> </ol>"},{"location":"roteiro4/main/","title":"4. Metrics","text":"<p>Se chegou aqui, \u00e9 porque voc\u00ea est\u00e1 interessado em saber mais. Logo, de brinde, como rodar um c\u00f3digo <code>Python</code> aqui.</p> 2025-10-06T16:42:33.353740 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ 2025-10-06T16:42:35.291092 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p>Markdown-exec \u00e9 uma extens\u00e3o do Markdown que permite executar c\u00f3digo Python diretamente no Markdown. Isso \u00e9 \u00fatil para gerar resultados din\u00e2micos ou executar scripts de forma interativa.</p>"}]}